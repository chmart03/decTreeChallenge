---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
---

# ğŸŒ³ Decision Tree Challenge - Feature Importance and Variable Encoding

## The Decision Tree Problem ğŸ¯

**The Core Problem:** Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?

**What is Feature Importance?** In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It's a key metric for understanding which variables matter most for your predictions.

The key question is: **How does encoding categorical variables as numbers affect our understanding of feature importance?**

**The Problem: ZipCode as Numerical vs Categorical**

What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?

Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e. neighborhoods. When treated as numerical, the tree might split on "zipCode > 50012.5" - which has no meaningful interpretation for house prices. Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e. zip code 99999 is not the priceiest zip code).

## Discussion Questions

### 1. Numerical vs Categorical Encoding

Zip codes should be treated as categorical variables because the numbers themselves do not carry any meaningful relationship to home prices. A higher zip code does not imply a more expensive neighborhood, and a lower one does not imply a cheaper area. These values are simply labels used for mail routes.

A quick real-world example illustrates this clearly. In New York City:

- 10013 (TriBeCa / SoHo, Manhattan) is one of the most expensive ZIP codes, with median home prices in the multimillion-dollar range.

- 10454 (South Bronx) is significantly more affordable â€” even though 10454 is numerically larger than 10013.

This confirms that the numeric ordering of zip codes has no business meaning. Treating them as numbers can lead a decision tree to create splits like â€œzipCode > 10050,â€ which misrepresents how neighborhoods actually differ.

Because the digits in a zip code do not represent magnitude, rank, or distance, the appropriate modeling choice is to represent zip codes as non-ordinal categorical variables.

### 2. R vs Python Implementation Differences

R generally handles categorical variables more naturally in decision tree models. When a variable is designated as a factor, Râ€™s tree algorithms understand that it represents categories rather than numbers. This allows the model to create splits based on groups of categories (e.g., â€œthese neighborhoods vs. othersâ€), which aligns with how businesses think about real estate markets.

Pythonâ€™s scikit-learn implementation behaves differently. The DecisionTreeRegressor does not support categorical variables natively, which means that any categorical variable passed into the model is automatically treated as a number unless it is encoded first. This limitation is clearly stated in the official documentation:

> "The scikit-learn implementation does not support categorical variables for now."
> â€” sklearn.tree.DecisionTreeRegressor documentation

From a business perspective, this distinction matters. If a model mistakenly treats zip codes as numeric, it may infer relationships that do not reflect actual market behavior â€” such as assuming that â€œhigher zip codesâ€ correspond to higher prices, which is contradicted by the Manhattan vs. Bronx example above.

Therefore, R handles this scenario correctly by default, while Python requires additional preprocessing before the model can interpret zip codes meaningfully.

### 3. Suggestions for Implementing Decision Trees in Python With Proper Categorical Handling

A practical way to improve the handling of categorical variables such as zip codes in Python decision trees is through target encoding.

Target encoding works by replacing each category with a summary value of the target variable for that category. For example, each zip code could be replaced with the median sale price of homes in that area. This provides the model with a numeric value that captures the economic reality of each neighborhood, rather than the arbitrary numeric label of the zip code.

A clear explanation of this method comes from Letâ€™s Data Science:

> "Target encoding replaces categories with the mean value of the target variable for each category, creating a numeric feature that reflects the category's relationship to the outcome."
> â€” Target Encoding: Categories Guided by Outcomes, letsdatascience.com

This approach fits well with scikit-learnâ€™s DecisionTreeRegressor, which expects numerical inputs. By converting each zip code into a meaningful numeric representation â€” rather than its literal digits â€” the model can evaluate neighborhood differences appropriately.

Target encoding must be implemented carefully (typically within cross-validation) to avoid leaking information about the target variable. However, when done properly, it provides a structured, business-aligned way of incorporating categorical features into Python decision trees, addressing the limitation described in the scikit-learn documentation:

â€œThe scikit-learn implementation does not support categorical variables for now.â€
